a1a=['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']
a1b=2649
a1c=12.05988565013656
a1d='function'
a2a=13
a2b=2.4630453700815003
a4a3=0.8689827219809665
a4b1=[('Tooling', 'ADV'), ('through', 'ADP'), ('Sydney', 'NOUN'), ('on', 'ADP'), ('his', 'DET'), ('way', 'NOUN'), ('to', 'ADP'), ('race', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('New', 'ADJ'), ('Zealand', 'X'), ('Grand', 'X'), ('Prix', 'X')]
a4b2=[('Tooling', 'ADV'), ('through', 'ADP'), ('Sydney', 'NOUN'), ('on', 'ADP'), ('his', 'DET'), ('way', 'NOUN'), ('to', 'ADP'), ('race', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('New', 'NOUN'), ('Zealand', 'NOUN'), ('Grand', 'X'), ('Prix', 'X')]
a4b3="In our model, the probaility of'New' being used as an adjective after the article 'the' is higher than the probability oF being used as a personal noun. As the algorithm does not capture longer word sequences, some potentially useful distinctions are lost, such as named-entity re"
a4c=56.630575309531835
a4d=308.71227854747974
a4e=['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADV']
a5='To ensure that the grammar produces a parse for any well-formed sentence, we consider 2 cases: For unseen words and ambiguities, we use the pre-trained POS tagger to find the most likely tag bases on the transition probability. For any other word, we use the output generated by the original parser (using a tree of the sentence based on the POS instead of the actual words).'
a6='The Universal tag set comprises of 12 tag classes, whereas the Brown Corpus consists of 87. We use the first one in order to get more precise estimates for both the emission and transition probabilities. Using the latter would result in less accurate probabilities, as the possibility of each word taking 87 tags would result in many emission and transition probabilities close to 0 and hence, many errors in the POS tag.'
a3c=16.79319240474419
a3d='<s>'
